#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Pandoc Wordæ–‡æ¡£å¤„ç†å·¥å…· - å¢å¼ºç‰ˆ (æ”¯æŒåŠ ç‚¹å­—æ£€æµ‹ï¼Œå›¾ç‰‡å¤„ç†å·²ç¦ç”¨)

ä½¿ç”¨pandocå°†Wordæ–‡æ¡£è½¬æ¢ä¸ºæ¨¡å‹å¯è¯»çš„çº¯æ–‡æœ¬å†…å®¹ï¼Œæ”¯æŒï¼š
1. æ–‡æ¡£æ–‡æœ¬è½¬æ¢ (Pandoc)
2. å¤§æ¨¡å‹APIè°ƒç”¨ (æ–‡æ¡£ç»“æ„è§£æ)
3. ç€é‡å·æ£€æµ‹ (åŠ ç‚¹å­—æ ‡è®°ä¿ç•™)
4. è¿ç»­çŸ­æ¨ªçº¿è½¬ä¸­æ–‡ç ´æŠ˜å·

æ³¨ï¼šå›¾ç‰‡æå–å’Œå†…å®¹åˆ†æåŠŸèƒ½å·²ç¦ç”¨ä»¥æé«˜è¿è¡Œæ•ˆç‡
æ³¨ï¼šåŠ ç‚¹å­—è½¬æ¢ä¸ºHTMLåŠŸèƒ½å·²ç§»é™¤ï¼Œæ ‡è®°ä¿ç•™ä¾›åç»­æ¨¡å‹å¤„ç†

ä¾èµ–å®‰è£…ï¼š
1. ç¡®ä¿ç³»ç»Ÿå·²å®‰è£…pandoc: https://pandoc.org/installing.html
2. å®‰è£…python-docx: pip install python-docx (ä»…ç”¨äºåŠ ç‚¹å­—é¢„å¤„ç†)
3. å®‰è£…å…¶ä»–ä¾èµ–: pip install requests

ä½¿ç”¨æ–¹æ³•ï¼š
1. è¿è¡Œè„šæœ¬å¤„ç†Wordæ–‡æ¡£
2. æ£€æµ‹å¹¶ä¿ç•™åŠ ç‚¹å­—æ ¼å¼æ ‡è®°
3. ç”Ÿæˆæœ€ç»ˆçš„è§£æç»“æœ
"""

import subprocess
import requests
import json
import time
import os
import re
import zipfile
import tempfile
from datetime import datetime
from pathlib import Path
from PIL import Image
import base64
from io import BytesIO
from collections import defaultdict

# ç‰¹æ®Šæ ¼å¼è¯†åˆ«ä¾èµ–
try:
    from docx import Document
    from docx.enum.text import WD_UNDERLINE
    from docx.oxml.ns import qn

    DOCX_AVAILABLE = True
    print("âœ… python-docxåº“å¯ç”¨ï¼Œæ”¯æŒç‰¹æ®Šæ ¼å¼è¯†åˆ«")
except ImportError:
    DOCX_AVAILABLE = False
    print("âš ï¸ python-docxåº“ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ç‰¹æ®Šæ ¼å¼è¯†åˆ«åŠŸèƒ½")
    print("   å®‰è£…å‘½ä»¤: pip install python-docx")


class PandocWordProcessor:
    def __init__(self):
        self.api_key = "baf9ea42-7e17-4df6-9a22-90127ac8220e"
        self.base_url = "https://ark.cn-beijing.volces.com/api"

        def _check_pandoc(self):
            """æ£€æŸ¥pandocæ˜¯å¦å¯ç”¨"""
            try:
                result = subprocess.run(['pandoc', '--version'],
                                        capture_output=True, text=True, timeout=10)
                if result.returncode == 0:
                    print(f"âœ… Pandocå¯ç”¨: {result.stdout.split()[1]}")
                    return True
                else:
                    print(f"âŒ Pandocæ£€æŸ¥å¤±è´¥: {result.stderr}")
                    return False
            except FileNotFoundError:
                print("âŒ æœªæ‰¾åˆ°pandocå‘½ä»¤")
                return False
            except subprocess.TimeoutExpired:
                print("âŒ Pandocæ£€æŸ¥è¶…æ—¶")
                return False
            except Exception as e:
                print(f"âŒ Pandocæ£€æŸ¥å¼‚å¸¸: {e}")
                return False

        # æ£€æŸ¥pandocæ˜¯å¦å¯ç”¨
        self.pandoc_available = _check_pandoc(self)
        if not self.pandoc_available:
            print("âš ï¸ è­¦å‘Š: pandocæœªå®‰è£…æˆ–ä¸åœ¨PATHä¸­")
            print("è¯·è®¿é—® https://pandoc.org/installing.html å®‰è£…pandoc")

        def _init_format_styles(self):
            """åˆå§‹åŒ–æ ¼å¼æ ·å¼æ˜ å°„"""
            if not DOCX_AVAILABLE:
                return

            # ä¸‹åˆ’çº¿æ ·å¼æ˜ å°„ - å®‰å…¨åœ°æ·»åŠ ä¸‹åˆ’çº¿æ ·å¼
            self.underline_styles = {}
            styles_to_add = [
                (getattr(WD_UNDERLINE, 'SINGLE', None), "å•ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'DOUBLE', None), "åŒä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'THICK', None), "ç²—ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'DOTTED', None), "ç‚¹çŠ¶ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'DASH', None), "è™šçº¿ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'DOT_DASH', None), "ç‚¹åˆ’çº¿ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'DOT_DOT_DASH', None), "ç‚¹ç‚¹åˆ’çº¿ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'WAVY', None), "æ³¢æµªçº¿ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'DOTTED_HEAVY', None), "ç²—ç‚¹çŠ¶ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'DASH_HEAVY', None), "ç²—è™šçº¿ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'WAVY_HEAVY', None), "ç²—æ³¢æµªçº¿ä¸‹åˆ’çº¿"),
                (getattr(WD_UNDERLINE, 'WAVY_DOUBLE', None), "åŒæ³¢æµªçº¿ä¸‹åˆ’çº¿")
            ]

            for style_enum, style_name in styles_to_add:
                if style_enum is not None:
                    self.underline_styles[style_enum] = style_name

            print(f"ğŸ“‹ åˆå§‹åŒ–äº† {len(self.underline_styles)} ç§ä¸‹åˆ’çº¿æ ·å¼è¯†åˆ«")

        # ç‰¹æ®Šæ ¼å¼è¯†åˆ«åŠŸèƒ½åˆå§‹åŒ–
        self.format_detection_enabled = DOCX_AVAILABLE
        self.special_formatted_text = []
        self.format_statistics = defaultdict(int)
        self.paragraph_formatting = []  # æ–°å¢ï¼šå­˜å‚¨æ®µè½æ ¼å¼ä¿¡æ¯

        if self.format_detection_enabled:
            _init_format_styles(self)

    def _analyze_text_formatting(self, run, para_index=0, run_index=0):
        """å·¥å…·å‡½æ•°ï¼šåˆ†ææ–‡æœ¬ç‰‡æ®µçš„æ ¼å¼"""
        if not DOCX_AVAILABLE:
            return []

        formats = []
        font = run.font

        def _check_emphasis_mark(self, run):
            """å·¥å…·å‡½æ•°ï¼šæ£€æŸ¥ç€é‡å·ï¼ˆåŠ ç‚¹å­—ï¼‰"""
            try:
                run_xml = run._element
                em_elements = run_xml.xpath('.//w:em', namespaces={
                    'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'})

                if em_elements:
                    em_val = em_elements[0].get(qn('w:val'))
                    emphasis_types = {
                        'dot': 'ç‚¹',
                        'comma': 'é€—å·',
                        'circle': 'åœ†åœˆ',
                        'underDot': 'ä¸‹ç‚¹'
                    }
                    return emphasis_types.get(em_val, f'ç€é‡å·({em_val})')
            except:
                pass

            return None

        # ä¸‹åˆ’çº¿æ£€æŸ¥
        if font.underline:
            underline_style = self.underline_styles.get(font.underline, f"æœªçŸ¥ä¸‹åˆ’çº¿æ ·å¼({font.underline})")
            formats.append(f"ä¸‹åˆ’çº¿: {underline_style}")

            # ç‰¹åˆ«æ ‡è®°æ³¢æµªçº¿å’Œç‚¹çŠ¶çº¿
            wavy_styles = [style for style in [
                getattr(WD_UNDERLINE, 'WAVY', None),
                getattr(WD_UNDERLINE, 'WAVY_HEAVY', None),
                getattr(WD_UNDERLINE, 'WAVY_DOUBLE', None)
            ] if style is not None]

            dotted_styles = [style for style in [
                getattr(WD_UNDERLINE, 'DOTTED', None),
                getattr(WD_UNDERLINE, 'DOTTED_HEAVY', None)
            ] if style is not None]

            if font.underline in wavy_styles:
                formats.append("âš ï¸ æ³¢æµªçº¿æ ¼å¼")
            elif font.underline in dotted_styles:
                formats.append("âš ï¸ ç‚¹çŠ¶çº¿æ ¼å¼")

        # ä¸Šæ ‡ä¸‹æ ‡
        if font.superscript:
            formats.append("ä¸Šæ ‡")
        if font.subscript:
            formats.append("ä¸‹æ ‡")

        # åˆ é™¤çº¿
        if font.strike:
            formats.append("åˆ é™¤çº¿")

        # ç²—ä½“æ–œä½“
        if font.bold:
            formats.append("ç²—ä½“")
        if font.italic:
            formats.append("æ–œä½“")

        # å­—ä½“é¢œè‰²
        if font.color and font.color.rgb:
            try:
                rgb_val = font.color.rgb
                if hasattr(rgb_val, '__int__'):
                    color_hex = f"#{int(rgb_val):06x}"
                else:
                    color_hex = str(rgb_val)
                formats.append(f"å­—ä½“é¢œè‰²: {color_hex}")
            except Exception:
                formats.append("å­—ä½“é¢œè‰²: ç‰¹æ®Šé¢œè‰²")

        # å­—ä½“å¤§å°
        if font.size:
            try:
                size_pt = font.size.pt
                formats.append(f"å­—å·: {size_pt}ç£…")
            except:
                formats.append("å­—å·: è‡ªå®šä¹‰å¤§å°")

        # å­—ä½“åç§°
        if font.name:
            formats.append(f"å­—ä½“: {font.name}")

        # æ£€æŸ¥ç€é‡å·
        emphasis_mark = _check_emphasis_mark(self, run)
        if emphasis_mark:
            formats.append(f"ç€é‡å·: {emphasis_mark}")

        # ç»Ÿè®¡æ ¼å¼ä½¿ç”¨
        for fmt in formats:
            self.format_statistics[fmt] += 1

        # ä¿å­˜ç‰¹æ®Šæ ¼å¼çš„æ–‡æœ¬
        if formats and run.text.strip():
            self.special_formatted_text.append({
                'text': run.text,
                'paragraph': para_index,
                'run': run_index,
                'formats': formats
            })

        return formats

    def _analyze_paragraph_formatting(self, paragraph, para_index=0):
        """åˆ†ææ®µè½çš„æ ¼å¼ï¼ŒåŒ…æ‹¬é¦–è¡Œç¼©è¿›"""
        if not DOCX_AVAILABLE:
            return []

        para_formats = []

        try:
            # è·å–æ®µè½æ ¼å¼
            para_format = paragraph.paragraph_format

            # æ£€æŸ¥é¦–è¡Œç¼©è¿›
            if para_format.first_line_indent:
                indent_value = para_format.first_line_indent
                # è½¬æ¢ä¸ºç£…æ•°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                try:
                    indent_pt = indent_value.pt if hasattr(indent_value, 'pt') else None
                    if indent_pt and indent_pt > 0:
                        para_formats.append(f"é¦–è¡Œç¼©è¿›: {indent_pt:.1f}ç£…")
                        # æ ‡è®°è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„æ ¼å¼ä¿¡æ¯
                        para_formats.append("âš ï¸ é¦–è¡Œç¼©è¿›æ®µè½")
                except:
                    para_formats.append("é¦–è¡Œç¼©è¿›: è‡ªå®šä¹‰å€¼")
                    para_formats.append("âš ï¸ é¦–è¡Œç¼©è¿›æ®µè½")

            # æ£€æŸ¥å·¦ç¼©è¿›
            if para_format.left_indent:
                try:
                    left_indent_pt = para_format.left_indent.pt if hasattr(para_format.left_indent, 'pt') else None
                    if left_indent_pt and left_indent_pt > 0:
                        para_formats.append(f"å·¦ç¼©è¿›: {left_indent_pt:.1f}ç£…")
                except:
                    para_formats.append("å·¦ç¼©è¿›: è‡ªå®šä¹‰å€¼")

            # æ£€æŸ¥å³ç¼©è¿›
            if para_format.right_indent:
                try:
                    right_indent_pt = para_format.right_indent.pt if hasattr(para_format.right_indent, 'pt') else None
                    if right_indent_pt and right_indent_pt > 0:
                        para_formats.append(f"å³ç¼©è¿›: {right_indent_pt:.1f}ç£…")
                except:
                    para_formats.append("å³ç¼©è¿›: è‡ªå®šä¹‰å€¼")

            # æ£€æŸ¥å¯¹é½æ–¹å¼
            if para_format.alignment:
                alignment_names = {
                    0: "å·¦å¯¹é½",
                    1: "å±…ä¸­",
                    2: "å³å¯¹é½",
                    3: "ä¸¤ç«¯å¯¹é½",
                    4: "åˆ†æ•£å¯¹é½"
                }
                alignment_name = alignment_names.get(para_format.alignment, f"å¯¹é½æ–¹å¼{para_format.alignment}")
                para_formats.append(f"å¯¹é½: {alignment_name}")

            # ç»Ÿè®¡æ ¼å¼ä½¿ç”¨
            for fmt in para_formats:
                self.format_statistics[fmt] += 1

        except Exception as e:
            print(f"  âš ï¸ æ®µè½æ ¼å¼åˆ†æå¤±è´¥: {e}")

        return para_formats

    def extract_format_analysis(self, docx_path):
        """æå–æ–‡æ¡£çš„æ ¼å¼åˆ†æä¿¡æ¯"""
        if not self.format_detection_enabled:
            print("ğŸš« æ ¼å¼æ£€æµ‹åŠŸèƒ½æœªå¯ç”¨ï¼Œè·³è¿‡æ ¼å¼åˆ†æ")
            return None

        print("ğŸ” å¼€å§‹åˆ†ææ–‡æ¡£æ ¼å¼...")

        try:
            doc = Document(docx_path)
            paragraph_count = 0

            # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
            self.special_formatted_text = []
            self.format_statistics = defaultdict(int)
            self.paragraph_formatting = []  # é‡ç½®æ®µè½æ ¼å¼ä¿¡æ¯

            for para in doc.paragraphs:
                paragraph_count += 1

                # åˆ†ææ®µè½æ ¼å¼ï¼ˆé¦–è¡Œç¼©è¿›ç­‰ï¼‰
                para_formats = self._analyze_paragraph_formatting(para, paragraph_count)
                if para_formats:
                    # ä¿å­˜æ®µè½æ ¼å¼ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ®µè½æ–‡æœ¬
                    para_text = para.text.strip()
                    if para_text:  # åªä¿å­˜éç©ºæ®µè½
                        self.paragraph_formatting.append({
                            'paragraph_index': paragraph_count,
                            'text': para_text,
                            'formats': para_formats,
                            'has_first_line_indent': any('é¦–è¡Œç¼©è¿›' in fmt for fmt in para_formats),
                            'is_centered': any('å¯¹é½: å±…ä¸­' in fmt for fmt in para_formats),
                            'is_right_aligned': any('å¯¹é½: å³å¯¹é½' in fmt for fmt in para_formats)
                        })

                # åˆ†ææ®µè½ä¸­çš„æ–‡æœ¬æ ¼å¼
                for run_index, run in enumerate(para.runs):
                    self._analyze_text_formatting(run, paragraph_count, run_index)

            # åˆ†æè¡¨æ ¼
            table_count = 0
            for table in doc.tables:
                table_count += 1
                for row_index, row in enumerate(table.rows):
                    for cell_index, cell in enumerate(row.cells):
                        for para in cell.paragraphs:
                            for run_index, run in enumerate(para.runs):
                                location = f"è¡¨æ ¼{table_count}-è¡Œ{row_index}-åˆ—{cell_index}"
                                self._analyze_text_formatting(run, location, run_index)

            # ç»Ÿè®¡é¦–è¡Œç¼©è¿›æ®µè½ã€å±…ä¸­æ®µè½å’Œå±…å³æ®µè½
            indent_paragraphs = len([p for p in self.paragraph_formatting if p['has_first_line_indent']])
            centered_paragraphs = len([p for p in self.paragraph_formatting if p['is_centered']])
            right_aligned_paragraphs = len([p for p in self.paragraph_formatting if p['is_right_aligned']])

            print(f"âœ… æ ¼å¼åˆ†æå®Œæˆ: {paragraph_count}ä¸ªæ®µè½, {table_count}ä¸ªè¡¨æ ¼")
            print(f"ğŸ“Š å‘ç° {len(self.special_formatted_text)} ä¸ªåŒ…å«ç‰¹æ®Šæ ¼å¼çš„æ–‡æœ¬ç‰‡æ®µ")
            print(f"ğŸ“ å‘ç° {indent_paragraphs} ä¸ªåŒ…å«é¦–è¡Œç¼©è¿›çš„æ®µè½")
            print(f"ğŸ“ å‘ç° {centered_paragraphs} ä¸ªå±…ä¸­å¯¹é½çš„æ®µè½")
            print(f"ğŸ“‘ å‘ç° {right_aligned_paragraphs} ä¸ªå±…å³å¯¹é½çš„æ®µè½")

            return {
                'total_paragraphs': paragraph_count,
                'total_tables': table_count,
                'special_format_count': len(self.special_formatted_text),
                'paragraph_format_count': len(self.paragraph_formatting),
                'indent_paragraph_count': indent_paragraphs,
                'centered_paragraph_count': centered_paragraphs,
                'right_aligned_paragraph_count': right_aligned_paragraphs,
                'format_statistics': dict(self.format_statistics)
            }

        except Exception as e:
            print(f"âŒ æ ¼å¼åˆ†æå¤±è´¥: {e}")
            return None

    def preprocess(self, docx_path):
        print("ğŸ” é¢„å¤„ç†...")

        try:
            # å¯¼å…¥é¢„å¤„ç†å™¨
            import zipfile
            import xml.etree.ElementTree as ET
            import tempfile
            import shutil
            import re

            # åˆ›å»ºä¸“é—¨çš„å­æ–‡ä»¶å¤¹æ¥å­˜å‚¨ä¸­é—´æ–‡ä»¶
            from pathlib import Path
            input_path = Path(docx_path)

            # å¦‚æœæ–‡ä»¶åœ¨Chineseæ–‡ä»¶å¤¹ä¸­ï¼Œåˆ›å»ºprocessedå­æ–‡ä»¶å¤¹
            if 'Chinese' in str(input_path):
                # è·å–Chineseæ–‡ä»¶å¤¹çš„è·¯å¾„
                chinese_folder = None
                for parent in input_path.parents:
                    if parent.name == 'Chinese':
                        chinese_folder = parent
                        break

                if chinese_folder:
                    processed_folder = chinese_folder / 'processed'
                    processed_folder.mkdir(exist_ok=True)
                    filename = input_path.name.replace('.docx', '_dot_processed.docx')
                    output_path = str(processed_folder / filename)
                    print(f"ä¸­é—´æ–‡ä»¶å°†ä¿å­˜åˆ°: processed/{filename}")
                else:
                    # å›é€€åˆ°åŸæ¥çš„æ–¹å¼
                    output_path = docx_path.replace('.docx', '_dot_processed.docx')
            else:
                # ä¸åœ¨Chineseæ–‡ä»¶å¤¹ä¸­ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹å¼
                output_path = docx_path.replace('.docx', '_dot_processed.docx')

            # åˆ›å»ºä¸´æ—¶ç›®å½•æ¥è§£å‹å’Œé‡æ–°æ‰“åŒ…docx
            with tempfile.TemporaryDirectory() as temp_dir:
                extract_dir = Path(temp_dir) / "docx_content"
                extract_dir.mkdir()

                # è§£å‹docxæ–‡ä»¶
                with zipfile.ZipFile(docx_path, 'r') as zip_file:
                    zip_file.extractall(extract_dir)

                # ä¿®æ”¹document.xml
                document_xml_path = extract_dir / "word" / "document.xml"
                if document_xml_path.exists():
                    with open(document_xml_path, 'r', encoding='utf-8') as f:
                        xml_content = f.read()

                    # æŸ¥æ‰¾å¹¶æ›¿æ¢åŠ ç‚¹å­—æ ‡è®°
                    run_with_em_pattern = r'(<w:r>.*?<w:rPr>.*?)<w:em w:val="dot"\s*/>(.*?</w:rPr>.*?<w:t>)(.*?)(</w:t>.*?</w:r>)'

                    def replace_run_with_em(match):
                        before_em = match.group(1)
                        after_em = match.group(2)
                        text_content = match.group(3)
                        after_text = match.group(4)

                        # æ·»åŠ ä¸‹åˆ’çº¿å’Œç‰¹æ®Šæ ‡è®°
                        underline_xml = '<w:u w:val="single"/>'
                        marked_text = f"[DOT_BELOW]{text_content}[/DOT_BELOW]"

                        return f"{before_em}{underline_xml}{after_em}{marked_text}{after_text}"

                    # ğŸŒŠ å®‰å…¨çš„æ³¢æµªçº¿XMLé¢„å¤„ç†ï¼ˆä¿®æ­£ç‰ˆï¼‰
                    modified_content = xml_content

                    # ğŸ”§ ä¿®å¤ï¼šå…è®¸rPrå†…æœ‰å…¶ä»–æ ‡ç­¾ï¼Œä½†ç¡®ä¿åœ¨åŒä¸€ä¸ªw:rå†…
                    # å…³é”®ï¼šåªåŒ¹é…æœ‰xml:space="preserve"çš„waveæ ¼å¼ï¼ˆçœŸæ­£çš„å¡«ç©ºï¼‰
                    wavy_pattern = r'(<w:r><w:rPr>(?:[^<]|<[^/][^>]*>)*<w:u w:val="wave"/>(?:[^<]|<[^/][^>]*>)*</w:rPr><w:t[^>]*xml:space="preserve"[^>]*>)(\s+)(</w:t></w:r>)'

                    def replace_wavy_spaces(match):
                        before_text = match.group(1)  # <w:r><w:rPr>...<w:t>
                        spaces = match.group(2)  # ç©ºæ ¼å†…å®¹
                        after_text = match.group(3)  # </w:t></w:r>

                        # ğŸ” ä¸¥æ ¼æ£€æŸ¥ï¼šåªå¤„ç†çº¯ç©ºæ ¼çš„æ³¢æµªçº¿ï¼ˆçœŸæ­£çš„å¡«ç©ºï¼‰
                        if spaces.strip() != '':
                            # å¦‚æœä¸æ˜¯çº¯ç©ºæ ¼ï¼Œåˆ™ä¸å¤„ç†ï¼Œä¿æŒåŸæ ·
                            return match.group(0)

                        space_count = len(spaces)
                        # ä¿æŒXMLç»“æ„å®Œæ•´ï¼Œåªæ›¿æ¢æ–‡æœ¬å†…å®¹
                        marked_text = f"[WAVY_SPACE_{space_count}]"
                        return f"{before_text}{marked_text}{after_text}"

                    # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥æ³¢æµªçº¿åŒ¹é…å’Œå¤„ç†è¿‡ç¨‹
                    test_matches = re.findall(wavy_pattern, modified_content, flags=re.DOTALL)
                    print(f"  ğŸ” è°ƒè¯•ï¼šæ‰¾åˆ° {len(test_matches)} ä¸ªæ³¢æµªçº¿æ¨¡å¼")
                    for i, match in enumerate(test_matches):
                        content = match[1]
                        is_pure_space = content.strip() == ''
                        print(
                            f"    åŒ¹é… {i + 1}: å†…å®¹é•¿åº¦={len(content)}, çº¯ç©ºæ ¼={is_pure_space}, å†…å®¹='{content[:20]}...'")


                # é‡æ–°æ‰“åŒ…docxæ–‡ä»¶
                with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for file_path in extract_dir.rglob('*'):
                        if file_path.is_file():
                            relative_path = file_path.relative_to(extract_dir)
                            zip_file.write(file_path, relative_path)

            return output_path

        except Exception as e:
            print(f"  âš ï¸ é¢„å¤„ç†å¤±è´¥: {e}")
            return None

    def convert_word_to_text(self, file_path, output_format='markdown'):
        """ä½¿ç”¨pandocå°†Wordæ–‡æ¡£è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œå¹¶å¢å¼ºæ ¼å¼æ ‡æ³¨"""
        if not self.pandoc_available:
            print("âŒ Pandocä¸å¯ç”¨ï¼Œæ— æ³•å¤„ç†æ–‡æ¡£")
            return None

        if not os.path.exists(file_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return None

        print(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_path}")
        print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / (1024 * 1024):.2f} MB")

        try:
            # æ„å»ºpandocå‘½ä»¤
            import shutil
            from pathlib import Path

            # è·å–è¯•å·åï¼ˆå»æ‰æ‰©å±•åï¼‰
            exam_name = Path(file_path).stem
            exam_media_dir = Path("media") / f"{exam_name}_media"

            # å¦‚æœæ–‡ä»¶å¤¹å­˜åœ¨ï¼Œæ¸…ç©ºå†…å®¹
            if exam_media_dir.exists():
                shutil.rmtree(exam_media_dir)
                print(f"ğŸ—‘ï¸ æ¸…ç©ºç°æœ‰å›¾ç‰‡æ–‡ä»¶å¤¹: {exam_media_dir}")
            exam_media_dir.mkdir(parents=True, exist_ok=True)
            print(f"ğŸ“ åˆ›å»ºå›¾ç‰‡æ–‡ä»¶å¤¹: {exam_media_dir}")

            cmd = [
                'pandoc',
                file_path,
                '--to', output_format,
                '--wrap', 'none',  # ä¸è‡ªåŠ¨æ¢è¡Œ
                '--standalone',  # ç”Ÿæˆç‹¬ç«‹æ–‡æ¡£
                '--extract-media', str(exam_media_dir),  # ç›´æ¥æå–åˆ°è¯•å·ä¸“ç”¨æ–‡ä»¶å¤¹
                '--quiet'  # å‡å°‘è¾“å‡º
            ]

            print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

            # æ‰§è¡Œpandocè½¬æ¢
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

            if result.returncode == 0:
                content = result.stdout
                print(f"âœ… è½¬æ¢æˆåŠŸ: {len(content)} å­—ç¬¦")

                # å¦‚æœæ˜¯docxæ–‡ä»¶ï¼Œè¿›è¡Œæ ¼å¼åˆ†æ
                if file_path.lower().endswith('.docx'):
                    print("æ£€æµ‹åˆ°docxæ–‡ä»¶ï¼Œå¼€å§‹æ ¼å¼åˆ†æ...")
                    # è¿›è¡Œæ ¼å¼åˆ†æ
                    format_analysis = self.extract_format_analysis(file_path)
                    if format_analysis:
                        print(f"âœ… æ ¼å¼åˆ†æå®Œæˆ: {len(self.paragraph_formatting)} ä¸ªæ®µè½æ ¼å¼")

                # æ–°å¢ï¼šå¦‚æœæœ‰æ ¼å¼åˆ†æç»“æœï¼Œå¢å¼ºpandocå†…å®¹
                if (hasattr(self, 'special_formatted_text') and self.special_formatted_text) or \
                        (hasattr(self, 'paragraph_formatting') and self.paragraph_formatting):
                    print("ğŸ¨ å¼€å§‹å¢å¼ºæ ¼å¼æ ‡æ³¨...")
                    content = self._enhance_content_with_format_info(content)

                # å¤„ç†å›¾ç‰‡è·¯å¾„ï¼šåˆ›å»ºè¯•å·åå­æ–‡ä»¶å¤¹å¹¶æ›´æ–°å›¾ç‰‡è·¯å¾„
                content = self._process_image_paths(content, file_path)

                # ä¿å­˜è½¬æ¢ç»“æœ
                pandoc_res_dir = Path("pandoc_res")
                pandoc_res_dir.mkdir(exist_ok=True)

                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_filename = pandoc_res_dir / f"pandocè½¬æ¢ç»“æœ_{timestamp}.txt"
                with open(output_filename, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"è½¬æ¢ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")

                return content
            else:
                print(f"âŒ è½¬æ¢å¤±è´¥: {result.stderr}")
                return None

        except subprocess.TimeoutExpired:
            print("âŒ è½¬æ¢è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰")
            return None
        except Exception as e:
            print(f"âŒ è½¬æ¢å¼‚å¸¸: {e}")
            return None

    def _process_image_paths(self, content, file_path):
        """æ›´æ–°å›¾ç‰‡è·¯å¾„åˆ°è¯•å·ä¸“ç”¨æ–‡ä»¶å¤¹"""
        import re
        from pathlib import Path

        # è·å–è¯•å·åï¼ˆå»æ‰æ‰©å±•åï¼‰
        exam_name = Path(file_path).stem
        print(f"ğŸ“ æ›´æ–°å›¾ç‰‡è·¯å¾„ï¼Œè¯•å·å: {exam_name}")

        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡å¼•ç”¨å¹¶æ›´æ–°è·¯å¾„
        image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        matches = re.findall(image_pattern, content)

        if matches:
            print(f"ğŸ–¼ï¸ å‘ç° {len(matches)} ä¸ªå›¾ç‰‡å¼•ç”¨")

            for alt_text, image_path in matches:
                # æ›´æ–°å›¾ç‰‡è·¯å¾„
                new_path = f"media/{exam_name}_media/{Path(image_path).name}"
                content = content.replace(image_path, new_path)
                print(f"  âœ… æ›´æ–°è·¯å¾„: {Path(image_path).name}")
        else:
            print("â„¹ï¸ æœªå‘ç°å›¾ç‰‡å¼•ç”¨")

        return content

    def _clean_dot_below_markers(self, text):
        """æ¸…ç†åŠ ç‚¹å­—æ ‡è®°ï¼Œç”¨äºåŒ¹é…æ¯”è¾ƒ"""
        import re

        # æ ¼å¼1ï¼šå®Œæ•´pandocæ ¼å¼ [\[DOT_BELOW\]å­—ç¬¦\[/DOT_BELOW\]]{.underline}
        pattern1 = r'\[\\\[DOT_BELOW\\\]([\u4e00-\u9fff]+)\\\[/DOT_BELOW\\\]\]\{\.underline\}'
        cleaned = re.sub(pattern1, r'\1', text)

        # æ ¼å¼2ï¼šç®€åŒ–æ ¼å¼ [DOT_BELOW]å­—ç¬¦[/DOT_BELOW] (æ”¯æŒå¤šä¸ªå­—ç¬¦)
        pattern2 = r'\[DOT_BELOW\]([\u4e00-\u9fff]+)\[/DOT_BELOW\]'
        cleaned = re.sub(pattern2, r'\1', cleaned)

        # æ ¼å¼3ï¼šå¤„ç†ä¸å®Œæ•´çš„DOT_BELOWæ ‡è®°ï¼ˆå¦‚æˆªæ–­çš„æ–‡æœ¬ï¼‰
        pattern3 = r'\[DOT_BELOW\]([\u4e00-\u9fff]*)\[/DOT.*?'
        cleaned = re.sub(pattern3, r'\1', cleaned)

        # æ ¼å¼4ï¼šæ¸…ç†å‰©ä½™çš„DOT_BELOWå¼€å§‹æ ‡è®°
        pattern4 = r'\[DOT_BELOW\]'
        cleaned = re.sub(pattern4, '', cleaned)

        return cleaned

    def _should_enable_detailed_debug(self, para_text):
        """
        é€šç”¨è°ƒè¯•æ¡ä»¶åˆ¤æ–­ï¼šåŸºäºæ–‡æœ¬ç‰¹å¾å’Œå¤æ‚åº¦å†³å®šæ˜¯å¦å¯ç”¨è¯¦ç»†è°ƒè¯•
        """
        # æ–‡æœ¬é•¿åº¦ç›¸å…³æ¡ä»¶
        text_length = len(para_text)
        if text_length < 5:  # è¿‡çŸ­æ–‡æœ¬é€šå¸¸ä¸éœ€è¦è¯¦ç»†è°ƒè¯•
            return False
        if text_length > 50:  # é•¿æ–‡æœ¬æ›´éœ€è¦è°ƒè¯•
            return True

        # åŒ…å«ç‰¹æ®Šæ ¼å¼æ ‡è®°
        special_markers = ['DOT_BELOW', 'ã€', 'ã€‘', '[', ']', '\\[', '\\]']
        if any(marker in para_text for marker in special_markers):
            return True

        # åŒ…å«å¤æ‚æ ‡ç‚¹æˆ–æ ¼å¼
        complex_chars = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©',
                         'â€œ', 'â€', 'â€˜', 'â€™', 'ï¼ˆ', 'ï¼‰', 'â€”â€”', 'â€¦']
        if any(char in para_text for char in complex_chars):
            return True

        # åŒ…å«å¼•å·æˆ–ç‰¹æ®Šç¬¦å·
        if 'â€œ' in para_text or "â€" in para_text or 'ã€Œ' in para_text or 'ã€' in para_text:
            return True

        # ä¸­ç­‰é•¿åº¦çš„æ–‡æœ¬ï¼Œæœ‰ä¸€å®šè°ƒè¯•ä»·å€¼
        if 20 <= text_length <= 50:
            return True

        return False

    def _has_high_text_similarity(self, text1, text2):
        """
        é€šç”¨æ–‡æœ¬ç›¸ä¼¼åº¦åˆ¤æ–­ï¼šåŸºäºå¤šç§æŒ‡æ ‡è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦
        """
        if not text1 or not text2:
            return False

        # é•¿åº¦ç›¸ä¼¼æ€§æ£€æŸ¥
        len1, len2 = len(text1), len(text2)
        if abs(len1 - len2) > max(len1, len2) * 0.5:  # é•¿åº¦å·®å¼‚è¶…è¿‡50%
            return False

        # å­—ç¬¦é›†é‡å æ£€æŸ¥
        chars1, chars2 = set(text1), set(text2)
        overlap = len(chars1 & chars2)
        union = len(chars1 | chars2)
        if union > 0:
            char_similarity = overlap / union
            if char_similarity > 0.7:  # å­—ç¬¦é‡å åº¦è¶…è¿‡70%
                return True

        # å­ä¸²åŒ…å«æ£€æŸ¥
        shorter, longer = (text1, text2) if len1 < len2 else (text2, text1)
        if len(shorter) >= 5:  # åªå¯¹æœ‰æ„ä¹‰é•¿åº¦çš„æ–‡æœ¬åšå­ä¸²æ£€æŸ¥
            # æ£€æŸ¥è¾ƒçŸ­æ–‡æœ¬çš„å‰åŠéƒ¨åˆ†æ˜¯å¦åœ¨è¾ƒé•¿æ–‡æœ¬ä¸­
            half_len = len(shorter) // 2
            if half_len >= 3 and shorter[:half_len] in longer:
                return True
            # æ£€æŸ¥è¾ƒçŸ­æ–‡æœ¬çš„ååŠéƒ¨åˆ†æ˜¯å¦åœ¨è¾ƒé•¿æ–‡æœ¬ä¸­
            if half_len >= 3 and shorter[-half_len:] in longer:
                return True

        # ä¸­æ–‡è¯æ±‡é‡å æ£€æŸ¥ï¼ˆé’ˆå¯¹ä¸­æ–‡æ–‡æ¡£ï¼‰
        import re
        chinese_words1 = re.findall(r'[\u4e00-\u9fff]{2,}', text1)
        chinese_words2 = re.findall(r'[\u4e00-\u9fff]{2,}', text2)

        if chinese_words1 and chinese_words2:
            word_overlap = len(set(chinese_words1) & set(chinese_words2))
            word_total = len(set(chinese_words1) | set(chinese_words2))
            if word_total > 0 and word_overlap / word_total > 0.5:  # è¯æ±‡é‡å è¶…è¿‡50%
                return True

        return False

    def _find_centered_text_match(self, para_text, content):
        """ä¸“é—¨ç”¨äºå±…ä¸­æ–‡æœ¬çš„åŒ¹é…æ–¹æ³•ï¼Œä¼˜å…ˆåŒ¹é…ç‹¬ç«‹è¡Œ"""
        lines = content.split('\n')

        # ä¼˜å…ˆåŒ¹é…ç‹¬ç«‹è¡Œ
        for line in lines:
            if line.strip() == para_text:
                return line.strip(), "ç‹¬ç«‹è¡Œ"

        # åå¤‡ï¼šä½¿ç”¨åŸæœ‰ç®—æ³•
        return self._find_best_match_in_content(para_text, content)

    def _normalize_quotes(self, text):
        """æ ‡å‡†åŒ–å¼•å·ï¼Œç”¨äºåŒ¹é…æ¯”è¾ƒ"""
        # å°†å„ç§ä¸­æ–‡å¼•å·ç»Ÿä¸€ä¸ºæ ‡å‡†å¼•å·
        quote_mappings = {
            'â€œ': '"',  # å·¦åŒå¼•å· (8220) -> æ™®é€šåŒå¼•å· (34)
            'â€': '"',  # å³åŒå¼•å· (8221) -> æ™®é€šåŒå¼•å· (34)
            'â€˜': "'",  # å·¦å•å¼•å· (8216) -> æ™®é€šå•å¼•å· (39)
            'â€™': "'",  # å³å•å¼•å· (8217) -> æ™®é€šå•å¼•å· (39)
            'ã€Œ': '"',  # æ—¥å¼å·¦å¼•å·
            'ã€': '"',  # æ—¥å¼å³å¼•å·
            'ã€': '"',  # æ—¥å¼å·¦åŒå¼•å·
            'ã€': '"',  # æ—¥å¼å³åŒå¼•å·
        }

        result = text
        for old_quote, new_quote in quote_mappings.items():
            result = result.replace(old_quote, new_quote)
        return result

    def _find_best_match_in_content(self, para_text, content):
        """åœ¨å†…å®¹ä¸­æ‰¾åˆ°æ®µè½çš„æœ€ä½³åŒ¹é…ä½ç½®"""

        # æ·»åŠ ç©ºæ ¼å¤„ç†
        para_text_cleaned = ' '.join(para_text.split())

        # ç‰¹æ®Šå¤„ç†ï¼šä¼˜å…ˆå°è¯•åŒ¹é…ç‹¬ç«‹è¡Œï¼ˆç‰¹åˆ«æ˜¯æ ‡é¢˜ç±»æ–‡æœ¬ï¼‰
        lines = content.split('\n')
        for line in lines:
            line_stripped = line.strip()
            line_cleaned = ' '.join(line_stripped.split())

            # å°è¯•ç›´æ¥åŒ¹é…
            if line_stripped == para_text or line_cleaned == para_text_cleaned:
                return line_stripped, "ç‹¬ç«‹è¡Œ"

            # å°è¯•æ ‡å‡†åŒ–ååŒ¹é…ç‹¬ç«‹è¡Œ
            normalized_line = self._normalize_quotes(line_stripped)
            normalized_para = self._normalize_quotes(para_text)
            normalized_line_cleaned = ' '.join(normalized_line.split())
            normalized_para_cleaned = ' '.join(normalized_para.split())

            if normalized_line_cleaned == normalized_para_cleaned:
                return line_stripped, "ç‹¬ç«‹è¡Œå¼•å·æ¸…ç†"

            # ğŸ”§ æ–°å¢ï¼šDOT_BELOWæ¸…ç†ååŒ¹é…
            cleaned_line = self._clean_dot_below_markers(line_stripped)
            cleaned_para = self._clean_dot_below_markers(para_text)
            if cleaned_line == cleaned_para:
                return line_stripped, "ç‹¬ç«‹è¡ŒDOT_BELOWæ¸…ç†"

            # ç»¼åˆå¤„ç†ï¼šDOT_BELOW + å¼•å· + ç©ºæ ¼
            both_cleaned_line = ' '.join(self._normalize_quotes(self._clean_dot_below_markers(line_stripped)).split())
            both_cleaned_para = ' '.join(self._normalize_quotes(self._clean_dot_below_markers(para_text)).split())
            if both_cleaned_line == both_cleaned_para:
                return line_stripped, "ç‹¬ç«‹è¡Œç»¼åˆæ¸…ç†"

            # ğŸ”§ æ–°å¢ï¼šå¯¹äºçŸ­æ–‡æœ¬ï¼ˆå¦‚æ ‡é¢˜ï¼‰ï¼Œæ£€æŸ¥æ˜¯å¦ä½œä¸ºç‹¬ç«‹è¡Œå­˜åœ¨
            if len(para_text) <= 20:  # æ ‡é¢˜é€šå¸¸è¾ƒçŸ­
                # æ£€æŸ¥è¯¥æ–‡æœ¬æ˜¯å¦ä½œä¸ºç‹¬ç«‹è¡Œå­˜åœ¨ï¼ˆå‰åéƒ½æ˜¯ç©ºè¡Œæˆ–æ¢è¡Œï¼‰
                if para_text in line_stripped and len(line_stripped) <= len(para_text) + 5:
                    # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯åŒ…å«åœ¨é•¿å¥å­ä¸­
                    if line_stripped == para_text or line_stripped.startswith(para_text) and len(line_stripped) - len(
                            para_text) <= 3:
                        return line_stripped, "ç‹¬ç«‹è¡ŒçŸ­æ–‡æœ¬"

        # ä¼˜åŒ–é•¿åº¦ç­–ç•¥ - å¯¹çŸ­æ–‡æœ¬æ›´çµæ´»
        if len(para_text) <= 8:
            # çŸ­æ–‡æœ¬ï¼šä¼˜å…ˆå®Œæ•´åŒ¹é…ï¼Œç„¶åé€æ­¥å‡å°‘
            lengths = [len(para_text)]
            if len(para_text) > 3:
                lengths.extend([len(para_text) - 1, len(para_text) - 2])
            if len(para_text) > 5:
                lengths.append(5)
        else:
            # é•¿æ–‡æœ¬ï¼šä½¿ç”¨æ›´å¤šé€‰é¡¹
            lengths = [25, 20, 15, 12, 10, 8]

        for length in lengths:
            if len(para_text) < length:
                continue

            para_start = para_text[:length]
            para_start_cleaned = ' '.join(para_start.split())

            # æ–¹æ³•1ï¼šç›´æ¥åŒ¹é…
            if para_start in content:
                if not any(f"ã€{marker}ã€‘{para_start}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                    return para_start, f"ç²¾ç¡®{length}"

            # æ–¹æ³•1.5ï¼šç©ºæ ¼æ¸…ç†ååŒ¹é…
            if para_start_cleaned != para_start and para_start_cleaned in content:
                if not any(f"ã€{marker}ã€‘{para_start_cleaned}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                    return para_start, f"ç©ºæ ¼æ¸…ç†{length}"

            # æ–¹æ³•2ï¼šæ ‡å‡†åŒ–å¼•å·ååŒ¹é…
            normalized_para_start = self._normalize_quotes(para_start)
            if normalized_para_start != para_start:
                if normalized_para_start in content:
                    if not any(
                            f"ã€{marker}ã€‘{normalized_para_start}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return normalized_para_start, f"å¼•å·{length}"

                # åŒæ—¶æ ‡å‡†åŒ–å¼•å·å’Œæ¸…ç†ç©ºæ ¼
                normalized_cleaned = ' '.join(normalized_para_start.split())
                if normalized_cleaned != normalized_para_start and normalized_cleaned in content:
                    if not any(f"ã€{marker}ã€‘{normalized_cleaned}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return para_start, f"å¼•å·ç©ºæ ¼{length}"

            # æ–¹æ³•3ï¼šæ¸…ç†åŠ ç‚¹å­—æ ‡è®°ååŒ¹é…
            cleaned_para_start = self._clean_dot_below_markers(para_start)
            if cleaned_para_start != para_start:
                if cleaned_para_start in content:
                    if not any(f"ã€{marker}ã€‘{cleaned_para_start}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return cleaned_para_start, f"æ¸…ç†{length}"

                # ğŸ”§ æ–°å¢ï¼šå¦‚æœç›´æ¥åœ¨contentä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•é€è¡ŒåŒ¹é…
                lines = content.split('\n')
                for line in lines:
                    line_cleaned = self._clean_dot_below_markers(line.strip())
                    if cleaned_para_start in line_cleaned:
                        if not any(f"ã€{marker}ã€‘{cleaned_para_start}" in content for marker in
                                   ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                            return cleaned_para_start, f"é€è¡Œæ¸…ç†{length}"

            # æ–¹æ³•4ï¼šç»¼åˆå¤„ç†ï¼ˆå¼•å·+åŠ ç‚¹å­—+ç©ºæ ¼ï¼‰
            both_processed = self._normalize_quotes(self._clean_dot_below_markers(para_start))
            both_processed_cleaned = ' '.join(both_processed.split())

            if both_processed != para_start:
                if both_processed in content:
                    if not any(f"ã€{marker}ã€‘{both_processed}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return both_processed, f"ç»¼åˆ{length}"

                if both_processed_cleaned != both_processed and both_processed_cleaned in content:
                    if not any(f"ã€{marker}ã€‘{both_processed_cleaned}" in content for marker in
                               ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                        return para_start, f"ç»¼åˆç©ºæ ¼{length}"

        # ç‰¹æ®Šå¤„ç†ï¼šåºå·æ®µè½ï¼ˆâ‘ â‘¡â‘¢â‘­ç­‰ï¼‰
        import re
        if re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]', para_text):
            content_without_number = para_text[1:].strip()
            for attempt_length in [len(content_without_number), min(10, len(content_without_number))]:
                if len(content_without_number) >= attempt_length > 0:
                    text_to_find = content_without_number[:attempt_length]
                    if text_to_find in content:
                        if not any(f"ã€{marker}ã€‘{text_to_find}" in content for marker in ["é¦–è¡Œç¼©è¿›", "å±…ä¸­", "å±…å³"]):
                            return para_text, f"åºå·åŒ¹é…{attempt_length}"

        # ğŸ†• å›é€€åŒ¹é…ç­–ç•¥ï¼šæ›´å®½æ¾çš„åŒ¹é…ç®—æ³•
        # åªåœ¨è°ƒè¯•æ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        should_debug_fallback = self._should_enable_detailed_debug(para_text)
        if should_debug_fallback:
            print(f"     å°è¯•å›é€€åŒ¹é…ç­–ç•¥...")

        # å›é€€ç­–ç•¥1ï¼šæ¨¡ç³Šå­—ç¬¦åŒ¹é…
        para_chars = set(para_text)
        content_lines = content.split('\n')
        best_match = None
        best_score = 0

        for line in content_lines:
            line_stripped = line.strip()
            if len(line_stripped) < 5:  # è·³è¿‡è¿‡çŸ­çš„è¡Œ
                continue

            # è®¡ç®—å­—ç¬¦é‡å ç‡
            line_chars = set(line_stripped)
            overlap = len(para_chars & line_chars)
            total_chars = len(para_chars | line_chars)
            if total_chars > 0:
                score = overlap / total_chars

                # å¦‚æœé‡å ç‡å¾ˆé«˜ä¸”é•¿åº¦ç›¸è¿‘
                if score > 0.8 and abs(len(line_stripped) - len(para_text)) < max(5, len(para_text) * 0.2):
                    if score > best_score:
                        best_score = score
                        best_match = line_stripped

        if best_match:
            if should_debug_fallback:
                print(f"     âœ… æ¨¡ç³ŠåŒ¹é…æˆåŠŸ (ç›¸ä¼¼åº¦: {best_score:.2f})")
            return best_match, f"æ¨¡ç³ŠåŒ¹é…({best_score:.2f})"

        # å›é€€ç­–ç•¥2ï¼šå…³é”®è¯åŒ¹é…
        # æå–ä¸­æ–‡å­—ç¬¦ä½œä¸ºå…³é”®è¯
        chinese_chars = re.findall(r'[\u4e00-\u9fff]+', para_text)
        if chinese_chars:
            # å–æœ€é•¿çš„ä¸­æ–‡è¯æ±‡ä½œä¸ºå…³é”®è¯
            key_phrase = max(chinese_chars, key=len)
            if len(key_phrase) >= 3:  # è‡³å°‘3ä¸ªå­—
                for line in content_lines:
                    line_stripped = line.strip()
                    if key_phrase in line_stripped and len(line_stripped) > len(key_phrase):
                        # æ£€æŸ¥ä¸Šä¸‹æ–‡ç›¸ä¼¼æ€§
                        context_before = para_text[:para_text.find(key_phrase)]
                        context_after = para_text[para_text.find(key_phrase) + len(key_phrase):]

                        line_key_pos = line_stripped.find(key_phrase)
                        line_before = line_stripped[:line_key_pos]
                        line_after = line_stripped[line_key_pos + len(key_phrase):]

                        # ç®€å•çš„ä¸Šä¸‹æ–‡åŒ¹é…
                        before_match = any(c in line_before for c in context_before[-3:]) if context_before else True
                        after_match = any(c in line_after for c in context_after[:3]) if context_after else True

                        if before_match and after_match:
                            if should_debug_fallback:
                                print(f"     âœ… å…³é”®è¯åŒ¹é…æˆåŠŸ (å…³é”®è¯: {key_phrase})")
                            return line_stripped, f"å…³é”®è¯åŒ¹é…({key_phrase})"

        # å›é€€ç­–ç•¥3ï¼šæ•°å­—åºå·åŒ¹é…
        number_match = re.match(r'^([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©â‘ªâ‘«â‘¬â‘­â‘®â‘¯â‘°â‘±â‘²â‘³]|[0-9]+[\.ã€])', para_text)
        if number_match:
            number_prefix = number_match.group(0)
            remaining_text = para_text[len(number_prefix):].strip()

            for line in content_lines:
                line_stripped = line.strip()
                if remaining_text and len(remaining_text) > 3 and remaining_text[:10] in line_stripped:
                    if should_debug_fallback:
                        print(f"     âœ… åºå·å†…å®¹åŒ¹é…æˆåŠŸ")
                    return line_stripped, "åºå·å†…å®¹åŒ¹é…"

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•éƒ¨åˆ†åŒ¹é…ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        escaped_text = re.escape(para_text[:min(5, len(para_text))])
        if re.search(escaped_text, content):
            return None, "éƒ¨åˆ†å­˜åœ¨ä½†æ— æ³•åŒ¹é…"

        if should_debug_fallback:
            print(f"     âŒ æ‰€æœ‰åŒ¹é…ç­–ç•¥éƒ½å¤±è´¥")
        return None, None

    def _enhance_content_with_format_info(self, content):
        """æ ¹æ®æ ¼å¼åˆ†æç»“æœå¢å¼ºpandocå†…å®¹"""
        print(f"ğŸ” å¼€å§‹åˆ†æ {len(self.special_formatted_text)} ä¸ªç‰¹æ®Šæ ¼å¼æ–‡æœ¬")
        print(f"ğŸ“ å¼€å§‹åˆ†æ {len(self.paragraph_formatting)} ä¸ªæ®µè½æ ¼å¼")

        # ğŸŒŠ é¦–å…ˆï¼šè¯†åˆ«XMLé¢„å¤„ç†æ ‡è®°çš„æ³¢æµªçº¿å¡«ç©ºï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        import re
        format_enhanced_count = 0

        # è¯†åˆ«XMLé¢„å¤„ç†é˜¶æ®µæ’å…¥çš„æ³¢æµªçº¿ç©ºæ ¼æ ‡è®°ï¼ˆæ”¯æŒpandocè½¬ä¹‰ï¼‰
        wavy_space_pattern = r'\\?\[WAVY_SPACE_(\d+)\\?\]'
        wavy_space_matches = re.findall(wavy_space_pattern, content)

        if wavy_space_matches:
            print(f"  ğŸŒŠ å‘ç° {len(wavy_space_matches)} ä¸ªXMLé¢„å¤„ç†çš„æ³¢æµªçº¿å¡«ç©ºæ ‡è®°")

            def replace_wavy_space_marker(match):
                space_count = int(match.group(1))  # æå–çœŸå®çš„ç©ºæ ¼æ•°é‡
                nbsp_content = "&nbsp;" * space_count
                enhanced_text = f"[{nbsp_content}]{{.wavy-underline}}"
                print(f"  ğŸŒŠ è½¬æ¢æ³¢æµªçº¿å¡«ç©ºï¼šåŸå§‹{space_count}ä¸ªç©ºæ ¼ â†’ {space_count}ä¸ª&nbsp;")
                return enhanced_text

            # ä¿®æ­£ï¼šä½¿ç”¨å®Œæ•´æ¨¡å¼åŒ¹é…æ›¿æ¢ï¼Œå¤„ç†è½¬ä¹‰çš„æ–¹æ‹¬å·
            content = re.sub(wavy_space_pattern, replace_wavy_space_marker, content)
            format_enhanced_count += len(wavy_space_matches)
        else:
            # å¤‡ç”¨ï¼šåŸºäºæ ¼å¼åˆ†æçš„æ³¢æµªçº¿å¡«ç©ºæ£€æµ‹
            wavy_space_texts = []
            for item in self.special_formatted_text:
                if any("æ³¢æµªçº¿æ ¼å¼" in fmt for fmt in item['formats']):
                    text = item['text']
                    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯ç©ºæ ¼æ–‡æœ¬ï¼ˆå¯èƒ½çš„å¡«ç©ºï¼‰
                    if text.replace(' ', '').replace('\u00A0', '') == '':
                        wavy_space_texts.append((text, len(text)))
                        print(f"  ğŸŒŠ æ ¼å¼åˆ†æå‘ç°æ³¢æµªçº¿å¡«ç©ºï¼š{len(text)}ä¸ªç©ºæ ¼")

            # å¤„ç†æ£€æµ‹åˆ°çš„æ³¢æµªçº¿å¡«ç©º
            for original_text, space_count in wavy_space_texts:
                nbsp_content = "&nbsp;" * space_count
                enhanced_text = f"[{nbsp_content}]{{.wavy-underline}}"
                print(f"  ğŸŒŠ è½¬æ¢æ³¢æµªçº¿å¡«ç©ºï¼š{space_count}ä¸ª&nbsp;")
                content = content.replace(original_text, enhanced_text, 1)
                format_enhanced_count += 1

            if not wavy_space_texts:
                print(f"  âš ï¸ æœªæ£€æµ‹åˆ°æ³¢æµªçº¿å¡«ç©ºï¼Œè·³è¿‡å¤„ç†")

        # âš ï¸ æ³¨æ„ï¼šåˆ é™¤é‡å¤çš„æ³¢æµªçº¿å¤„ç†é€»è¾‘ï¼Œä½¿ç”¨é€šç”¨çš„æ ¼å¼æ ‡æ³¨å¤„ç†æœºåˆ¶

        # ğŸš¨ é‡è¦ï¼šä¼˜å…ˆå¤„ç†æ®µè½æ ¼å¼ï¼ˆå±…å³ã€å±…ä¸­ã€é¦–è¡Œç¼©è¿›ï¼‰ï¼Œé¿å…è¢«ç‰¹æ®Šæ ¼å¼æ–‡æœ¬å¤„ç†å¹²æ‰°
        # ç¬¬ä¸€æ­¥ï¼šå¤„ç†å±…å³æ®µè½
        right_aligned_enhanced_count = 0
        for para_info in self.paragraph_formatting:
            if para_info['is_right_aligned']:
                para_text = para_info['text'].strip()
                print(f"ğŸ” æ£€æŸ¥å±…å³æ–‡æœ¬: \"{para_text}\" (é•¿åº¦: {len(para_text)})")

                # è·³è¿‡è¿‡çŸ­çš„æ–‡æœ¬
                if len(para_text) < 1:
                    continue

                # ä½¿ç”¨æ”¹è¿›çš„åŒ¹é…ç®—æ³•
                match_result, match_type = self._find_best_match_in_content(para_text, content)

                if match_result:
                    # åœ¨åŒ¹é…çš„æ–‡æœ¬å‰æ·»åŠ æ ‡è®°
                    enhanced_start = f"ã€å±…å³ã€‘{match_result}"
                    content = content.replace(match_result, enhanced_start, 1)
                    right_aligned_enhanced_count += 1
                    print(f"âœ… å±…å³æ ‡è®°({match_type}): \"{match_result[:30]}...\"")
                else:
                    print(f"âŒ å±…å³æœªåŒ¹é…: \"{para_text[:30]}...\" (é•¿åº¦: {len(para_text)})")

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†å±…ä¸­æ®µè½
        centered_enhanced_count = 0
        for para_info in self.paragraph_formatting:
            if para_info['is_centered']:
                para_text = para_info['text'].strip()

                # è·³è¿‡è¿‡çŸ­çš„æ–‡æœ¬
                if len(para_text) < 2:
                    continue

                # é¿å…é‡å¤æ ‡è®°ï¼ˆå¦‚æœå·²ç»æœ‰å±…å³æ ‡è®°ï¼‰
                check_lengths = [min(10, len(para_text)), min(8, len(para_text))] if len(para_text) > 5 else [
                    len(para_text)]
                if any(f"ã€å±…å³ã€‘{para_text[:length]}" in content for length in check_lengths if
                       len(para_text) >= length):
                    print(f"  â†’ è·³è¿‡ï¼šå·²æœ‰å±…å³æ ‡è®°")
                    continue

                # ğŸ”§ ä¼˜åŒ–ï¼šå¯¹äºå±…ä¸­æ–‡æœ¬ï¼Œä¼˜å…ˆå¯»æ‰¾ç‹¬ç«‹è¡ŒåŒ¹é…
                match_result, match_type = self._find_centered_text_match(para_text, content)

                if match_result:
                    # å¯¹äºç‹¬ç«‹è¡Œï¼Œä½¿ç”¨è¡Œçº§åˆ«æ›¿æ¢
                    if match_type == "ç‹¬ç«‹è¡Œ":
                        lines = content.split('\n')
                        for i, line in enumerate(lines):
                            if line.strip() == match_result:
                                lines[i] = f"ã€å±…ä¸­ã€‘{line}"
                                content = '\n'.join(lines)
                                break
                    else:
                        # å…¶ä»–æƒ…å†µä½¿ç”¨æ™®é€šæ›¿æ¢
                        enhanced_start = f"ã€å±…ä¸­ã€‘{match_result}"
                        content = content.replace(match_result, enhanced_start, 1)

                    centered_enhanced_count += 1
                    print(f"âœ… å±…ä¸­æ ‡è®°({match_type}): \"{match_result[:30]}...\"")
                else:
                    print(f"âŒ å±…ä¸­æœªåŒ¹é…: \"{para_text[:20]}...\"")

        # ç¬¬ä¸‰æ­¥ï¼šå¤„ç†æ®µè½é¦–è¡Œç¼©è¿›ï¼ˆæœ€åå¤„ç†ï¼Œé¿å…è¯¯æŠ¢å…¶ä»–æ ¼å¼ï¼‰
        indent_enhanced_count = 0
        for para_info in self.paragraph_formatting:
            if para_info['has_first_line_indent']:
                para_text = para_info['text'].strip()

                # è·³è¿‡è¿‡çŸ­çš„æ–‡æœ¬
                if len(para_text) < 8:
                    continue

                # é¿å…é‡å¤æ ‡è®°ï¼ˆå¦‚æœå·²ç»æœ‰å…¶ä»–æ ‡è®°ï¼‰
                check_lengths = [min(10, len(para_text)), min(8, len(para_text))] if len(para_text) > 5 else [
                    len(para_text)]
                if any(f"ã€å±…å³ã€‘{para_text[:length]}" in content for length in check_lengths if
                       len(para_text) >= length):
                    print(f"  â†’ è·³è¿‡ï¼šå·²æœ‰å±…å³æ ‡è®°")
                    continue
                if any(f"ã€å±…ä¸­ã€‘{para_text[:length]}" in content for length in check_lengths if
                       len(para_text) >= length):
                    print(f"  â†’ è·³è¿‡ï¼šå·²æœ‰å±…ä¸­æ ‡è®°")
                    continue

                # ä½¿ç”¨æ”¹è¿›çš„åŒ¹é…ç®—æ³•
                match_result, match_type = self._find_best_match_in_content(para_text, content)

                if match_result:
                    # åœ¨åŒ¹é…çš„æ–‡æœ¬å‰æ·»åŠ æ ‡è®°
                    enhanced_start = f"ã€é¦–è¡Œç¼©è¿›ã€‘{match_result}"
                    content = content.replace(match_result, enhanced_start, 1)
                    indent_enhanced_count += 1
                    print(f"âœ… ç¼©è¿›æ ‡è®°({match_type}): \"{match_result[:30]}...\"")
                else:
                    print(f"âŒ ç¼©è¿›æœªåŒ¹é…: \"{para_text[:30]}...\"")
                    # é€šç”¨è°ƒè¯•æ¡ä»¶ï¼šåŸºäºæ–‡æœ¬ç‰¹å¾å’Œå¤æ‚åº¦åˆ¤æ–­æ˜¯å¦éœ€è¦è¯¦ç»†è°ƒè¯•
                    should_debug = self._should_enable_detailed_debug(para_text)
                    if should_debug:
                        print(f"  â†’ è¯¦ç»†è°ƒè¯•åŒ¹é…è¿‡ç¨‹:")
                        print(f"     åŸæ–‡æœ¬: {repr(para_text[:80])}")
                        print(f"     æ–‡æœ¬é•¿åº¦: {len(para_text)}")

                        # æ˜¾ç¤ºå„ç§æ¸…ç†æ­¥éª¤
                        cleaned_text = self._clean_dot_below_markers(para_text)
                        normalized_text = self._normalize_quotes(para_text)
                        fully_cleaned = self._normalize_quotes(cleaned_text)

                        print(f"     DOT_BELOWæ¸…ç†å: {repr(cleaned_text[:80])}")
                        print(f"     å¼•å·æ ‡å‡†åŒ–å: {repr(normalized_text[:80])}")
                        print(f"     å®Œå…¨æ¸…ç†å: {repr(fully_cleaned[:80])}")

                        # æ£€æŸ¥å„ç§åŒ¹é…å¯èƒ½æ€§
                        content_lines = content.split('\n')
                        found_similar = []

                        for i, line in enumerate(content_lines):
                            line_stripped = line.strip()
                            if not line_stripped:
                                continue

                            # æ£€æŸ¥å„ç§åŒ¹é…ï¼ˆç§»é™¤ç¡¬ç¼–ç çš„å†…å®¹åˆ¤æ–­ï¼‰
                            matches = []
                            if cleaned_text in line_stripped:
                                matches.append("DOT_BELOWæ¸…ç†")
                            if fully_cleaned in line_stripped:
                                matches.append("å®Œå…¨æ¸…ç†")
                            if para_text[:20] in line_stripped:
                                matches.append("å‰20å­—ç¬¦")
                            if line_stripped[:20] in para_text:
                                matches.append("è¡Œå‰20å­—ç¬¦")

                            # é€šç”¨ç›¸ä¼¼åº¦æ£€æŸ¥ï¼Œç§»é™¤å…·ä½“å†…å®¹åˆ¤æ–­
                            if matches or self._has_high_text_similarity(para_text, line_stripped):
                                found_similar.append(
                                    f"     ç¬¬{i + 1}è¡Œ: {repr(line_stripped[:80])} [{', '.join(matches) if matches else 'é«˜ç›¸ä¼¼åº¦'}]")

                        if found_similar:
                            print(f"     æ‰¾åˆ°ç›¸ä¼¼å†…å®¹:")
                            for similar in found_similar[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                                print(similar)
                        else:
                            print(f"     âŒ æœªæ‰¾åˆ°ä»»ä½•ç›¸ä¼¼å†…å®¹")

                        # å°è¯•æ¨¡ç³ŠåŒ¹é…
                        for j, line in enumerate(content_lines):
                            line_stripped = line.strip()
                            if len(line_stripped) > 10 and abs(len(line_stripped) - len(para_text)) < 10:
                                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å­—ç¬¦é‡å ï¼‰
                                overlap = len(set(para_text) & set(line_stripped))
                                if overlap > len(para_text) * 0.7:
                                    print(
                                        f"     ğŸ” é«˜ç›¸ä¼¼åº¦è¡Œ: {repr(line_stripped[:60])} (é‡å å­—ç¬¦: {overlap}/{len(para_text)})")
                                    cleaned_line = self._clean_dot_below_markers(line)
                                    print(f"     ç¬¬{j + 1}è¡Œæ¸…ç†å: {repr(cleaned_line[:80])}")

        # æŒ‰æ–‡æœ¬é•¿åº¦æ’åºï¼Œä»é•¿åˆ°çŸ­ï¼Œé¿å…çŸ­æ–‡æœ¬æ›¿æ¢å½±å“é•¿æ–‡æœ¬
        sorted_formats = sorted(self.special_formatted_text,
                                key=lambda x: len(x['text']), reverse=True)

        format_enhanced_count = 0

        for format_item in sorted_formats:
            text = format_item['text'].strip()
            formats = format_item['formats']

            # è·³è¿‡ç©ºæ–‡æœ¬æˆ–è¿‡çŸ­æ–‡æœ¬
            if len(text) < 2:
                continue

            # è·³è¿‡åŒ…å«å±…ä¸­æ–‡æœ¬çš„ç‰¹æ®Šæ ¼å¼
            for para_info in self.paragraph_formatting:
                if para_info['is_centered'] and para_info['text'] in text and para_info['text'] != text:
                    print(f"  â­ï¸ è·³è¿‡åŒ…å«å±…ä¸­æ–‡æœ¬çš„ç‰¹æ®Šæ ¼å¼: \"{text[:30]}...\"")
                    continue

            # ç”Ÿæˆæ ¼å¼æ ‡æ³¨
            format_annotation = self._generate_format_annotation(formats)

            # ğŸ”§ å­—ç¬¦æ ‡å‡†åŒ–ï¼šå¤„ç†pandocè½¬æ¢å¯¼è‡´çš„å­—ç¬¦å·®å¼‚
            # ä¸­æ–‡å¼•å· â†’ è‹±æ–‡å¼•å·ï¼Œä¸­æ–‡çœç•¥å· â†’ è‹±æ–‡ç‚¹å·
            normalized_text = text.replace('â€œ', '"').replace('â€', '"').replace('â€¦â€¦', '......')

            # ğŸ›¡ï¸ é˜²é‡å¤å¤„ç†ï¼šæ£€æŸ¥æ–‡æœ¬æ˜¯å¦å·²ç»è¢«æ ‡è®°è¿‡
            if format_annotation:

                # æ™ºèƒ½åŒ¹é…ï¼šå…ˆå°è¯•åŸå§‹æ–‡æœ¬ï¼Œå†å°è¯•æ ‡å‡†åŒ–æ–‡æœ¬
                target_text = None
                if text in content:
                    target_text = text
                elif normalized_text in content:
                    target_text = normalized_text

                if target_text:
                    # æ£€æŸ¥æ˜¯å¦å·²è¢«å±…ä¸­æ ‡è®°
                    if f"ã€å±…ä¸­ã€‘{target_text}" in content:
                        print(f"  â­ï¸ è·³è¿‡å·²è¢«å±…ä¸­æ ‡è®°çš„æ–‡æœ¬: \"{target_text[:30]}...\"")
                        continue

                    # åˆ›å»ºå¢å¼ºçš„æ–‡æœ¬æ ‡æ³¨
                    enhanced_text = f"[{target_text}]{{{format_annotation}}}"
                    content = content.replace(target_text, enhanced_text, 1)
                    format_enhanced_count += 1

                    if target_text != text:
                        print(
                            f"ğŸ”§ å­—ç¬¦æ ‡å‡†åŒ–æˆåŠŸ: \"{text[:30]}{'...' if len(text) > 30 else ''}\" -> {format_annotation}")
                    else:
                        print(f"æ ¼å¼å¢å¼º: \"{text[:30]}{'...' if len(text) > 30 else ''}\" -> {format_annotation}")

        print(f"âœ… æ ¼å¼å¢å¼ºå®Œæˆ:")
        print(f" å±…å³å¯¹é½æ ‡è®°: {right_aligned_enhanced_count} ä¸ªæ®µè½")
        print(f" å±…ä¸­å¯¹é½æ ‡è®°: {centered_enhanced_count} ä¸ªæ®µè½")
        print(f" é¦–è¡Œç¼©è¿›æ ‡è®°: {indent_enhanced_count} ä¸ªæ®µè½")
        print(f" ç‰¹æ®Šæ ¼å¼æ ‡è®°: {format_enhanced_count} ä¸ªæ–‡æœ¬")
        return content

    def _generate_format_annotation(self, formats):
        """æ ¹æ®æ ¼å¼åˆ—è¡¨ç”Ÿæˆæ ‡æ³¨"""
        annotations = []

        # ğŸŒŠ ç‰¹æ®Šå¤„ç†ï¼šæ³¢æµªçº¿æ ¼å¼ä¼˜å…ˆï¼Œåªè¿”å›æ³¢æµªçº¿æ ‡è®°
        for fmt in formats:
            if "æ³¢æµªçº¿æ ¼å¼" in fmt:
                return ".wavy-underline"  # ç›´æ¥è¿”å›ï¼Œä¸æ·»åŠ å…¶ä»–æ ¼å¼

        # å¤„ç†å…¶ä»–ä¸‹åˆ’çº¿ç±»å‹
        for fmt in formats:
            if "ç‚¹çŠ¶çº¿æ ¼å¼" in fmt:
                annotations.append(".dotted-underline")
            elif "ä¸‹åˆ’çº¿: å•ä¸‹åˆ’çº¿" in fmt:
                annotations.append(".single-underline")
            elif "ä¸‹åˆ’çº¿: åŒä¸‹åˆ’çº¿" in fmt:
                annotations.append(".double-underline")
            elif "ä¸‹åˆ’çº¿: ç²—ä¸‹åˆ’çº¿" in fmt:
                annotations.append(".thick-underline")
            elif "ä¸‹åˆ’çº¿: è™šçº¿ä¸‹åˆ’çº¿" in fmt:
                annotations.append(".dashed-underline")
            elif "åˆ é™¤çº¿" in fmt:
                annotations.append(".strikethrough")
            elif "ä¸Šæ ‡" in fmt:
                annotations.append(".superscript")
            elif "ä¸‹æ ‡" in fmt:
                annotations.append(".subscript")
            elif "ç€é‡å·" in fmt:
                annotations.append(".emphasis-mark")
            elif "ç²—ä½“" in fmt:
                annotations.append(".bold")
            elif "æ–œä½“" in fmt:
                annotations.append(".italic")

        # å¤„ç†å­—ä½“é¢œè‰²ï¼ˆæå–é¢œè‰²å€¼ï¼‰
        for fmt in formats:
            if "å­—ä½“é¢œè‰²:" in fmt and "000000" not in fmt:  # è·³è¿‡é»‘è‰²
                color = fmt.split("å­—ä½“é¢œè‰²:")[-1].strip()
                annotations.append(f".color-{color}")

        # å¤„ç†å­—å·
        for fmt in formats:
            if "å­—å·:" in fmt and "ç£…" in fmt:
                size = fmt.split("å­—å·:")[-1].replace("ç£…", "").strip()
                try:
                    size_num = float(size)
                    if size_num != 12.0:  # è·³è¿‡é»˜è®¤å­—å·
                        annotations.append(f".font-{size}pt")
                except:
                    pass

        return " ".join(annotations) if annotations else None

    def _convert_dashes_to_chinese(self, content):
        """è½¬æ¢è¿ç»­çŸ­æ¨ªçº¿ä¸ºä¸­æ–‡ç ´æŠ˜å·"""
        print("ğŸ”€ è½¬æ¢è¿ç»­çŸ­æ¨ªçº¿ä¸ºä¸­æ–‡ç ´æŠ˜å·...")

        try:
            import re

            conversion_count = 0

            dash_pattern = r'-{3,}'  # åŒ¹é…3ä¸ªæˆ–æ›´å¤šè¿ç»­çš„çŸ­æ¨ªçº¿

            def replace_dashes(match):
                nonlocal conversion_count
                dashes = match.group(0)
                dash_count = len(dashes)
                conversion_count += 1
                # æ¯3ä¸ªçŸ­æ¨ªçº¿æ›¿æ¢ä¸ºä¸€ä¸ªem dash
                em_dash_count = dash_count // 3
                return 'â€”' * em_dash_count

            content = re.sub(dash_pattern, replace_dashes, content)

            if conversion_count > 0:
                print(f"  âœ… è½¬æ¢äº† {conversion_count} å¤„è¿ç»­çŸ­æ¨ªçº¿ä¸ºä¸­æ–‡ç ´æŠ˜å·ï¼ˆæ¯3ä¸ªçŸ­æ¨ªçº¿è½¬æ¢ä¸º1ä¸ªem dashï¼‰")
            else:
                print("  â„¹ï¸ æœªå‘ç°éœ€è¦è½¬æ¢çš„è¿ç»­çŸ­æ¨ªçº¿")

            return content

        except Exception as e:
            print(f"  âš ï¸ ç ´æŠ˜å·è½¬æ¢å¤±è´¥: {e}")
            return content

    def call_llm_api(self, content, prompt_template_path="prompt.md"):
        """è°ƒç”¨å¤§æ¨¡å‹APIè§£ææ–‡æ¡£ç»“æ„"""
        print("å¼€å§‹è°ƒç”¨å¤§æ¨¡å‹API...")

        # è¯»å–promptæ¨¡æ¿
        try:
            with open(prompt_template_path, 'r', encoding='utf-8') as f:
                prompt_template = f.read()
            # ä½¿ç”¨å®‰å…¨çš„å­—ç¬¦ä¸²æ›¿æ¢
            prompt = prompt_template.replace("{content}", content)
            print(f"æˆåŠŸåŠ è½½promptæ¨¡æ¿: {prompt_template_path}")
        except FileNotFoundError:
            print(f"âŒ æœªæ‰¾åˆ°promptæ¨¡æ¿æ–‡ä»¶: {prompt_template_path}")
            print("âŒ è¿™æ˜¯ä¸€ä¸ªä¸¥é‡é”™è¯¯ï¼å¿…é¡»ä½¿ç”¨æ­£ç¡®çš„promptæ¨¡æ¿ï¼")
            print("âŒ é»˜è®¤promptä¸ä¼˜åŒ–åçš„è¦æ±‚ä¸åŒ¹é…ï¼Œä¼šå¯¼è‡´é€‰é¡¹ç¼ºå¤±ç­‰é—®é¢˜")
            print("ğŸ’¡ è¯·ç¡®ä¿prompt_Chinese.mdæ–‡ä»¶å­˜åœ¨ä¸”å¯è¯»")
            return None

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

        # å…ˆæµ‹è¯•APIè¿æ¥
        test_data = {
            "model": "doubao-seed-1-6-250615",
            "messages": [{"role": "user", "content": "Hello"}],
            "max_tokens": 32000,
            "temperature": 0.1,
            "stream": True,
            "thinking": {
                "type": "enabled",
                "budget_tokens": 2000
            }
        }

        print("ğŸ” æµ‹è¯•APIè¿æ¥...")
        try:
            test_response = requests.post(
                f"{self.base_url}/v3/chat/completions",
                headers=headers,
                json=test_data,
                timeout=30
            )
            test_response.raise_for_status()
            print("âœ… APIè¿æ¥æµ‹è¯•æˆåŠŸ")
        except requests.exceptions.HTTPError as e:
            print(f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            print(f"å“åº”çŠ¶æ€ç : {test_response.status_code}")
            print(f"å“åº”å†…å®¹: {test_response.text}")
            return None
        except Exception as e:
            print(f"âŒ APIè¿æ¥æµ‹è¯•å¼‚å¸¸: {e}")
            return None

        # æ­£å¼è¯·æ±‚ - ä½¿ç”¨æµå¼è¾“å‡º
        data = {
            "model": "doubao-seed-1-6-250615",
            "messages": [{"role": "user", "content": prompt}],
            "stream": True,
            "thinking": {
                "type": "enabled",
                "budget_tokens": 1500
            },
            "response_format": {
                "type": "json_object"
            },
            "temperature": 0.1,
            "max_completion_tokens": 32000
        }

        print("ğŸš€ è°ƒç”¨å¤§æ¨¡å‹APIè§£ææ–‡æ¡£ç»“æ„...")
        print(f"è¯·æ±‚URL: {self.base_url}/v3/chat/completions")
        print(f"æ¨¡å‹: {data['model']}")
        print(f"æ¶ˆæ¯é•¿åº¦: {len(prompt)} å­—ç¬¦")

        try:
            response = requests.post(
                f"{self.base_url}/v3/chat/completions",
                headers=headers,
                json=data,
                timeout=300,  # å‡å°‘è¶…æ—¶æ—¶é—´åˆ°5åˆ†é’Ÿ
                stream=True
            )
            response.raise_for_status()

            # å¤„ç†æµå¼å“åº”
            llm_content = ""
            print("ğŸ“¡ å¼€å§‹æ¥æ”¶æµå¼å“åº”...")

            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data_str = line[6:]  # å»æ‰ 'data: ' å‰ç¼€

                        if data_str == '[DONE]':
                            print("\nâœ… æµå¼å“åº”æ¥æ”¶å®Œæˆ")
                            break

                        try:
                            data_json = json.loads(data_str)
                            if 'choices' in data_json and len(data_json['choices']) > 0:
                                choice = data_json['choices'][0]

                                # å¤„ç†thinkingçŠ¶æ€
                                if 'thinking' in choice:
                                    thinking = choice['thinking']
                                    if thinking.get('type') == 'thinking':
                                        print(f"ğŸ¤” æ€è€ƒä¸­... ({thinking.get('tokens_used', 0)} tokens)")
                                    elif thinking.get('type') == 'finished':
                                        print(f"âœ… æ€è€ƒå®Œæˆï¼Œå…±ä½¿ç”¨ {thinking.get('tokens_used', 0)} tokens")

                                # å¤„ç†deltaå†…å®¹
                                if 'delta' in choice and 'content' in choice['delta']:
                                    content = choice['delta']['content']
                                    llm_content += content
                                    print(content, end='', flush=True)

                        except json.JSONDecodeError:
                            continue

            print(f"\nâœ… APIè°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(llm_content)} å­—ç¬¦")
            return llm_content

        except requests.exceptions.HTTPError as e:
            print(f"HTTPé”™è¯¯: {e}")
            print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
            print(f"å“åº”å†…å®¹: {response.text}")
            return None
        except Exception as e:
            print(f"âŒ APIè°ƒç”¨å¼‚å¸¸: {e}")
            return None


    def _process_api_response(self, llm_content, original_file_path):
        """å¤„ç†APIå“åº”å¹¶ä¿å­˜ç»“æœï¼Œå¢å¼ºå¥å£®æ€§å’Œé”™è¯¯å¤„ç†"""

        def extract_json_from_codeblock(content):
            """ä»Markdownä»£ç å—ä¸­æå–JSONå†…å®¹ï¼Œå¤„ç†å¤šç§æ ¼å¼æƒ…å†µ"""
            # åŒ¹é…```jsonå¼€å¤´çš„ä»£ç å—ï¼ˆå…è®¸å‰åæœ‰ç©ºç™½ï¼‰
            json_block_pattern = re.compile(r'^\s*```\s*json\s*\n(.*?)\n\s*```\s*$', re.DOTALL | re.MULTILINE)
            match = json_block_pattern.search(content)
            if match:
                return match.group(1).strip()

            # åŒ¹é…æ™®é€š```ä»£ç å—
            general_block_pattern = re.compile(r'^\s*```\s*\n(.*?)\n\s*```\s*$', re.DOTALL | re.MULTILINE)
            match = general_block_pattern.search(content)
            if match:
                return match.group(1).strip()

            # æ— ä»£ç å—æ—¶è¿”å›åŸå§‹å†…å®¹ï¼ˆå¯èƒ½å·²æ˜¯çº¯JSONï¼‰
            return content.strip()

        def clean_json_string(content):
            """æ¸…ç†JSONå­—ç¬¦ä¸²ï¼Œå¤„ç†å¸¸è§æ ¼å¼é—®é¢˜"""
            # ç§»é™¤JSONä¸­çš„æ³¨é‡Šï¼ˆ/* ... */ æˆ– // ...ï¼‰
            content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
            content = re.sub(r'//.*?$', '', content, flags=re.MULTILINE)

            # ç§»é™¤å°¾é€—å·ï¼ˆå¦‚ [1,2,] æˆ– {"a":1,}ï¼‰
            content = re.sub(r',\s*([}\]])', r'\1', content)

            return content.strip()

        # 1. æå–å¹¶æ¸…ç†å†…å®¹
        try:
            # æå–JSONå†…å®¹ï¼ˆå¤„ç†ä»£ç å—ï¼‰
            extracted_content = extract_json_from_codeblock(llm_content)

            # æ¸…ç†JSONæ ¼å¼é—®é¢˜
            cleaned_content = clean_json_string(extracted_content)
        except Exception as e:
            print(f"âš ï¸ å†…å®¹æå–/æ¸…ç†å¤±è´¥: {e}")
            cleaned_content = llm_content.strip()

        # 2. ä¿å­˜åŸå§‹å“åº”ï¼ˆæ— è®ºåç»­å¤„ç†æ˜¯å¦æˆåŠŸï¼Œä¾¿äºè°ƒè¯•ï¼‰
        try:
            # åˆ›å»ºä¸“é—¨çš„åŸå§‹å“åº”ç›®å½•
            raw_dir = Path("raw_api_responses")
            raw_dir.mkdir(exist_ok=True)

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            raw_filename = raw_dir / f"raw_response_{Path(original_file_path).stem}_{timestamp}.txt"
            with open(raw_filename, 'w', encoding='utf-8') as f:
                f.write(f"=== åŸå§‹LLMå“åº” ===\n{llm_content}\n\n")
                f.write(f"=== æå–åå†…å®¹ ===\n{extracted_content}\n\n")
                f.write(f"=== æ¸…ç†åå†…å®¹ ===\n{cleaned_content}")
            print(f"ğŸ“„ åŸå§‹å“åº”å·²ä¿å­˜åˆ°: {raw_filename}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åŸå§‹å“åº”å¤±è´¥: {e}")

        # 3. è§£æJSONå¹¶ä¿å­˜ç»“æœ
        try:
            questions = cleaned_content

            # éªŒè¯è§£æç»“æœæ ¼å¼
            if not isinstance(questions, list):
                raise ValueError("è§£æç»“æœä¸æ˜¯JSONæ•°ç»„")

            # ä¿å­˜å¤„ç†åçš„ç»“æœ
            json_res_dir = Path("json_res")
            json_res_dir.mkdir(exist_ok=True)

            output_file = json_res_dir / f"questions_{Path(original_file_path).stem}_{timestamp}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(questions, f, ensure_ascii=False, indent=2)

            print(f"ğŸ‰ å®Œæˆï¼å…±{len(questions)}é“é¢˜ç›®ï¼Œä¿å­˜åˆ°: {output_file}")
            return questions

        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"é”™è¯¯ä½ç½®: ç¬¬{e.lineno}è¡Œç¬¬{e.colno}åˆ—")
            return None
        except ValueError as e:
            print(f"âŒ è§£æç»“æœæ ¼å¼é”™è¯¯: {e}")
            return None
        except Exception as e:
            print(f"âŒ å¤„ç†å“åº”æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return None

    def call_coze_workflow(self, processed_data):
        """è°ƒç”¨Cozeå·¥ä½œæµ"""
        print("ğŸ”— å‡†å¤‡è°ƒç”¨Cozeå·¥ä½œæµ...")

        try:
            headers = {
                'Authorization': f'Bearer pat_Z0r3WQNZ435IUDhJCc0bVHDd9mVcIh0Z6tOvYd3HPT3Q6WNfw5KaX7veOhNkqC3N',
                'Content-Type': 'application/json'
            }

            data = {
                "workflow_id": "7540878860784680995",
                "parameters": {
                    "input": json.dumps(processed_data, ensure_ascii=False)
                }
            }

            print("ğŸš€ è°ƒç”¨Cozeå·¥ä½œæµå¼€å§‹...")
            print(f"ğŸ“Š å‘é€æ•°æ®é‡: {len(json.dumps(processed_data, ensure_ascii=False))} å­—ç¬¦")

            response = requests.post('https://api.coze.cn/v1/workflow/run', headers=headers, data=json.dumps(data))

            if response.status_code == 200:
                response_data = response.json().get("data")

                if response_data:
                    # è§£æ JSON å­—ç¬¦ä¸²
                    parsed_data = json.loads(response_data)

                    # æå– data å­—æ®µå¹¶æŒ‰ \n åˆ†å‰²æˆæ•°ç»„
                    id_list = parsed_data["data"].split("\n")

                    print(f"âœ… Cozeå·¥ä½œæµè°ƒç”¨æˆåŠŸï¼Œè¿”å› {len(id_list)} ä¸ªID")
                    print(f"ğŸ“‹ IDåˆ—è¡¨é¢„è§ˆ: {', '.join(id_list[:5])}...")  # åªæ˜¾ç¤ºå‰5ä¸ª

                    return id_list
                else:
                    print("âŒ Cozeå·¥ä½œæµè¿”å›æ•°æ®ä¸ºç©º")
                    return None
            else:
                print(f"âŒ Cozeå·¥ä½œæµè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                print(f"é”™è¯¯ä¿¡æ¯: {response.text}")
                return None

        except Exception as e:
            print(f"âŒ è°ƒç”¨Cozeå·¥ä½œæµå¼‚å¸¸: {e}")
            return None

    # ä¸»æµç¨‹å‡½æ•°
    def process_word_document(self, file_path, output_format='markdown', prompt_template_path="prompt.md",
                              enable_dot_below_detection=True, enable_coze_workflow=True):

        """æç¤ºæ€§ä¿¡æ¯"""
        print("=" * 60)
        print("Pandoc Wordæ–‡æ¡£å¤„ç†å·¥å…· - å¢å¼ºç‰ˆ (æ”¯æŒåŠ ç‚¹å­—)")
        print("=" * 60)
        print(f"æ–‡æ¡£æ–‡ä»¶: {file_path}")
        print(f"è¾“å‡ºæ ¼å¼: {output_format}")
        print(f"Promptæ¨¡æ¿: {prompt_template_path}")
        # print(f"æ ¼å¼åˆ†æ: å·²ç¦ç”¨")  # å·²ç§»é™¤æ ¼å¼åˆ†æåŠŸèƒ½
        print(f"åŠ ç‚¹å­—æ ‡è®°ä¿ç•™: {'å¯ç”¨' if enable_dot_below_detection else 'ç¦ç”¨'}")
        print(f"Cozeå·¥ä½œæµ: {'å¯ç”¨' if enable_coze_workflow else 'ç¦ç”¨'}")
        print("=" * 60)
        """æç¤ºç»“æŸ"""

        # ç¬¬ä¸€æ­¥ - åŠ ç‚¹å­—å’Œæ³¢æµªçº¿é¢„å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ä¸”ä¸ºdocxæ–‡ä»¶ï¼‰
        processed_file_path = file_path
        if enable_dot_below_detection and file_path.lower().endswith('.docx'):
            processed_file_path = self.preprocess(file_path)
            if not processed_file_path:
                processed_file_path = file_path  # å›é€€åˆ°åŸæ–‡ä»¶

        """æ‰“å°æ–‡æœ¬ä¸­ä¸€äº›æ ¼å¼ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼šç¼©è¿›ä¿¡æ¯ã€å¯¹é½æ–¹å¼ï¼ˆdebugç±»å‹çš„å‡½æ•°ï¼Œä¸åŠŸèƒ½æ— å…³ï¼‰"""
        # ç¬¬äºŒæ­¥ - æ ¼å¼åˆ†æï¼ˆå¦‚æœå¯ç”¨ä¸”ä¸ºdocxæ–‡ä»¶ï¼‰
        format_analysis = None
        if processed_file_path.lower().endswith('.docx'):
            format_analysis = self.extract_format_analysis(processed_file_path)

        # ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨pandocè½¬æ¢æ–‡æ¡£
        content = self.convert_word_to_text(processed_file_path, output_format)
        if not content:
            print("âŒ æ–‡æ¡£è½¬æ¢å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†")
            return None

        # ç¬¬å››æ­¥ï¼šè½¬æ¢è¿ç»­çŸ­æ¨ªçº¿ä¸ºä¸­æ–‡ç ´æŠ˜å·
        content = self._convert_dashes_to_chinese(content)

        # ç¬¬äº”æ­¥ï¼šè°ƒç”¨å¤§æ¨¡å‹APIè§£æå†…å®¹
        llm_response = self.call_llm_api(content, prompt_template_path)
        if not llm_response:
            print("âŒ APIè°ƒç”¨å¤±è´¥")
            return None

        # ç¬¬å…­æ­¥ï¼šå¤„ç†APIå“åº”å¹¶é›†æˆæ ¼å¼ä¿¡æ¯
        api_result = self._process_api_response(llm_response, file_path)

        # ç¬¬ä¸ƒæ­¥ï¼šè°ƒç”¨Cozeå·¥ä½œæµï¼ˆå¦‚æœå¯ç”¨ï¼‰
        coze_ids = None
        if enable_coze_workflow:
            print("\n" + "=" * 60)
            print("ğŸ”— Cozeå·¥ä½œæµå¤„ç†é˜¶æ®µ")
            print("=" * 60)

            if api_result:
                # æ­£å¸¸æƒ…å†µï¼šä½¿ç”¨APIè§£æç»“æœè°ƒç”¨Coze
                coze_ids = self.call_coze_workflow(api_result)

                if coze_ids:
                    # åˆ›å»ºcoze_resæ–‡ä»¶å¤¹
                    coze_res_dir = Path("coze_res")
                    coze_res_dir.mkdir(exist_ok=True)

                    # å°†Cozeè¿”å›çš„IDåˆ—è¡¨ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    coze_output_file = coze_res_dir / f"coze_ids_{Path(file_path).stem}_{timestamp}.txt"

                    with open(coze_output_file, 'w', encoding='utf-8') as f:
                        f.write(",".join(coze_ids))

                    print(f"ğŸ“ Coze IDåˆ—è¡¨å·²ä¿å­˜åˆ°: {coze_output_file}")

                    # å¯é€‰ï¼šå°†IDåˆ—è¡¨æ·»åŠ åˆ°APIç»“æœä¸­
                    if isinstance(api_result, list):
                        # å¦‚æœAPIç»“æœæ˜¯é¢˜ç›®åˆ—è¡¨ï¼Œå¯ä»¥ä¸ºæ¯é“é¢˜æ·»åŠ ä¸€ä¸ªID
                        for i, question in enumerate(api_result[:len(coze_ids)]):
                            if isinstance(question, dict):
                                question['coze_id'] = coze_ids[i] if i < len(coze_ids) else None
                    print("âœ… Coze IDå·²æ•´åˆåˆ°é¢˜ç›®ç»“æœä¸­")
                else:
                    print("âš ï¸ Cozeå·¥ä½œæµæœªè¿”å›æœ‰æ•ˆæ•°æ®")
            else:
                # APIè§£æå¤±è´¥çš„æƒ…å†µï¼šæä¾›æ‰‹åŠ¨è°ƒç”¨æŒ‡å¯¼
                print("âš ï¸ ç”±äºJSONè§£æå¤±è´¥ï¼Œæ— æ³•è‡ªåŠ¨è°ƒç”¨Cozeå·¥ä½œæµ")

        return {
            'questions': api_result,
            'coze_ids': coze_ids
        } if enable_coze_workflow else api_result


def main():
    """ä¸»å‡½æ•°"""
    import sys

    # é…ç½®å‚æ•°
    if len(sys.argv) > 1:
        word_file_path = sys.argv[1]
    else:
        word_file_path = "ç²¾å“è§£æï¼š2025å¹´å±±ä¸œçœæ³°å®‰å¸‚ä¸­è€ƒè‹±è¯­çœŸé¢˜ ï¼ˆè§£æç‰ˆï¼‰.docx"  # é»˜è®¤æ–‡ä»¶è·¯å¾„

    output_format = "markdown"  # å¯é€‰: markdown, plain, html
    prompt_template_path = "prompt_English.md"

    # åˆ›å»ºå¤„ç†å™¨å®ä¾‹
    processor = PandocWordProcessor()

    # æ£€æŸ¥pandocå¯ç”¨æ€§
    if not processor.pandoc_available:
        print("âŒ Pandocä¸å¯ç”¨ï¼Œè¯·å…ˆå®‰è£…pandoc")
        print("å®‰è£…æ–¹æ³•:")
        print("  macOS: brew install pandoc")
        print("  Ubuntu/Debian: sudo apt-get install pandoc")
        print("  Windows: ä¸‹è½½å®‰è£…åŒ… https://pandoc.org/installing.html")
        return

    # å¤„ç†æ–‡æ¡£
    result = processor.process_word_document(
        word_file_path,
        output_format,
        prompt_template_path
    )

    if result:
        print("âœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼")
    else:
        print("âŒ æ–‡æ¡£å¤„ç†å¤±è´¥")


if __name__ == "__main__":
    main()

"""

æ•´ä½“æ€è·¯ï¼šä¸‰å¤§æ¨¡å—ï¼š

    1. ä½¿ç”¨pandocåº“ä»¥åŠä¸€äº›å·¥å…·å‡½æ•°å¯¹ä¼ å…¥çš„wordè¿›è¡Œå„ç§æ ¼å¼è§£æå’Œå¤„ç†ï¼Œæœ€ç»ˆè¿”å›ä¸€ä¸ªmarkdownæ ¼å¼çš„æ–‡æœ¬ï¼ˆstringï¼‰

    2. è°ƒç”¨å¤§æ¨¡å‹APIï¼Œä¼ å…¥markdownæ ¼å¼çš„æ–‡æœ¬ï¼Œè¿”å›å¤§æ¨¡å‹å¯¹markdownæ–‡æœ¬å¤„ç†åçš„åŸå§‹ç›¸åº”å†…å®¹ï¼ˆstringï¼‰ï¼Œè°ƒç”¨_process_api_responseå‡½æ•°

    å¯¹åŸå§‹å“åº”å†…å®¹åšåå¤„ç†ï¼Œå¤„ç†æˆæœ€ç»ˆæ­£ç¡®æ ¼å¼çš„jsonå†…å®¹

    3. æŠŠç¬¬äºŒæ­¥çš„jsonä¿¡æ¯ï¼Œä¼ å…¥cozeå·¥ä½œæµè¿›è¡Œå¤„ç†ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢˜ç›®id


"""